
\documentclass[11pt]{article}
%\usepackage[top=20mm,left=20mm,right=20mm,bottom=15mm,a4paper]{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\usepackage[top=20mm,left=20mm,right=20mm,bottom=15mm,headsep=15pt,footskip=15pt,a4paper]{geometry} % see geometry.pdf on how to lay out the page. There's lots.
%\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry
\usepackage[round]{natbib}
\setlength{\bibsep}{0.0pt}
\usepackage{color}
\usepackage{times}
%\usepackage[T1]{fontenc}
%\usepackage{mathptmx}
\usepackage{tikz-dependency}
\usepackage{enumitem}
%\usepackage{times}

\usepackage[procnames]{listings}
\usepackage{color}
\usepackage{todonotes}
 
 

% See the ``Article customise'' template for come common customisations
\newcommand{\refeq}[1]{Equation~\ref{eq:#1}}
\newcommand{\reffig}[1]{Figure~\ref{fig:#1}}
\newcommand{\reftab}[1]{Table~\ref{tab:#1}}
\newcommand{\refsec}[1]{\textsection\ref{sec:#1}}
\newcommand{\newsec}[2]{\section{#1}\label{sec:#2}\noindent}
\newcommand{\newsubsec}[2]{\subsection{#1}\label{sec:#2}\noindent}
\newcommand{\argmax}{\operatornamewithlimits{argmax}} 
\newcommand{\argmin}{\operatornamewithlimits{argmin}} 

\makeatletter         
\def\@maketitle{   % custom maketitle 
\begin{center}%
{\bfseries \@title}%
{\bfseries \@author}%
\end{center}%
\smallskip \hrule \bigskip }

% custom section 
\renewcommand{\section}{\@startsection
{section}%                   % the name
{1}%                         % the level
{0mm}%                       % the indent
{-0.8\baselineskip}%            % the before skip
{0.3\baselineskip}%          % the after skip
{\bfseries\large}}% the style

% custom subsection 
\renewcommand{\subsection}{\@startsection
{subsection}%                   % the name
{2}%                         % the level
{0mm}%                       % the indent
{-0.8\baselineskip}%            % the before skip
{0.3\baselineskip}%          % the after skip
{\bfseries\large}}% the style

\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{1.5ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}\makeatother

%\title{{\LARGE Universal Parser (UP)}\\[-8mm]
%\includegraphics[height=8mm]{RUPA}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\includegraphics[height=8mm]{RUPA}}
\title{{\LARGE Natural Language Processing}\\[1.5mm]{\large Lab 1: Whitespace Tokenization}}
\author{}
\date{} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
 
\lstset{language=Python, 
        basicstyle=\ttfamily\small, 
        keywordstyle=\color{keywords},
        commentstyle=\color{comments},
        stringstyle=\color{red},
        showstringspaces=false,
        identifierstyle=\color{green},
        procnamekeys={def,class}}

\maketitle
%\tableofcontents
%\vspace{3mm}
\vspace{-2mm} \newsec{Introduction}{intro}%
In many languages, word boundaries are marked by spaces. However,
relying on whitespace alone seldom gives an adequate tokenization. In
this lab, you will inspect the output of a tokenizer that only
segments tokens separated by whitespace, discuss the shortcomings of
this approach, and start to think about how they can be overcome. In
the process, you will get acquainted with the Python programming
language (if you haven't used it before) \todo{can they come to this course
with no python??}.  All the files needed for
the lab are in {\tt /local/kurs/nlp/basic1/}. Start by copying
them into your home directory:\\
{\tt cd}\\
{\tt mkdir NLP\_Lab\_1}\\
{\tt cp -R /local/kurs/nlp/basic1/* NLP\_Lab\_1}\\
{\tt cd NLP\_Lab\_1}

\newsec{Tokenize some text}{tokenize}%
The file {\tt tokenizer0.py} contains the simplest possible tokenizer:
\begin{center}
\fbox{
\lstinputlisting{code/tokenizer0.py}
}
\end{center}
The first line imports the {\tt re} module (for regular expressions)
and the {\tt sys} module (for reading from stdin). The main program
consists of two nested {\tt for} loops: For each line in the input
file, split the line into tokens at (one or more) whitespace
characters ({\tt {$\backslash$}s+}), and print each token.  To run the
tokenizer on the file {\tt dev1-raw.txt} and print the result to the
(new) file {\tt dev1-tok.txt}, run the following command:
\begin{verbatim}
    python tokenizer0.py < dev1-raw.txt > dev1-tok.txt
\end{verbatim}
Check that {\tt dev1-tok.txt} contains a sequence of tokens, one on
each line, and you're good to go.

\newsec{Analyze the result}{analyse}%
Go through the output file and try to identify problems in the
tokenization. Use your intuition about what units are convenient for
further linguistic analysis and try to find arguments for why a
particular segmentation may lead to problems, for example, when we
perform a grammatical analysis or try to look up words in a
lexicon. (We are going to make this more precise next time, but it is
useful for you to create your own opinion first.)  Most problems you
encounter will be either cases of \emph{oversplitting}, where the
tokenizer has split what should have been one token into several
tokens, or \emph{undersplitting}, where the tokenizer has failed to
split a string into multiple tokens. Try to categorize different cases
of oversplitting and undersplitting and think about how they could be
eliminated.

\newsec{Correct the tokenization}{correct}%
When you have identified the tokenization errors, you should correct
them manually using a text editor. You should correct at least the
first ten lines of the text in {\tt dev1-raw.txt} (ending just before
the word ``Pacific'').


\end{document}
