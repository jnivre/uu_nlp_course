
\documentclass[11pt]{article}
%\usepackage[top=20mm,left=20mm,right=20mm,bottom=15mm,a4paper]{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\usepackage[top=20mm,left=20mm,right=20mm,bottom=15mm,headsep=15pt,footskip=15pt,a4paper]{geometry} % see geometry.pdf on how to lay out the page. There's lots.
%\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry
\usepackage[round]{natbib}
\setlength{\bibsep}{0.0pt}
\usepackage{color}
\usepackage{times}
%\usepackage[T1]{fontenc}
%\usepackage{mathptmx}
\usepackage{tikz-dependency}
\usepackage{enumitem}
%\usepackage{times}

\usepackage[procnames]{listings}
\usepackage{color}
 
 

% See the ``Article customise'' template for come common customisations
\newcommand{\refeq}[1]{Equation~\ref{eq:#1}}
\newcommand{\reffig}[1]{Figure~\ref{fig:#1}}
\newcommand{\reftab}[1]{Table~\ref{tab:#1}}
\newcommand{\refsec}[1]{\textsection\ref{sec:#1}}
\newcommand{\newsec}[2]{\section{#1}\label{sec:#2}\noindent}
\newcommand{\newsubsec}[2]{\subsection{#1}\label{sec:#2}\noindent}
\newcommand{\argmax}{\operatornamewithlimits{argmax}} 
\newcommand{\argmin}{\operatornamewithlimits{argmin}} 

\makeatletter         
\def\@maketitle{   % custom maketitle 
\begin{center}%
{\bfseries \@title}%
{\bfseries \@author}%
\end{center}%
\smallskip \hrule \bigskip }

% custom section 
\renewcommand{\section}{\@startsection
{section}%                   % the name
{1}%                         % the level
{0mm}%                       % the indent
{-0.8\baselineskip}%            % the before skip
{0.3\baselineskip}%          % the after skip
{\bfseries\large}}% the style

% custom subsection 
\renewcommand{\subsection}{\@startsection
{subsection}%                   % the name
{2}%                         % the level
{0mm}%                       % the indent
{-0.8\baselineskip}%            % the before skip
{0.3\baselineskip}%          % the after skip
{\bfseries\large}}% the style

\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{1.5ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}\makeatother

%\title{{\LARGE Universal Parser (UP)}\\[-8mm]
%\includegraphics[height=8mm]{RUPA}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\includegraphics[height=8mm]{RUPA}}
\title{{\LARGE Natural Language Processing}\\[1.5mm]{\large Assignment 10: Dependency Parsing}}
\author{}
\date{} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
 
\lstset{language=Python, 
        basicstyle=\ttfamily\small, 
        keywordstyle=\color{keywords},
        commentstyle=\color{comments},
        stringstyle=\color{red},
        showstringspaces=false,
        identifierstyle=\color{green},
        procnamekeys={def,class}}

\maketitle
%\tableofcontents
%\vspace{3mm}
\vspace{-2mm}
\newsec{Introduction}{intro}%
In this assignment, we will train and evaluate a transition-based dependency parser on the English web treebank. The input to parsing will be text that has been tokenized, tagged and lemmatized as in Assignments 2, 5 and 7, so by putting all these tools together you can build your own pipeline for grammatical analysis of English text. We will use the MaltParser system to build the parser and the MaltEval tool for evaluation and error analysis. 
\newsec{Data}{data}%
We will use the training and development set of the English web treebank, which is part of the Universal Dependencies treebanks.\footnote{{\tt http://universaldependencies.github.io/docs/}} The files can
be found in {\tt /local/kurs/nlp/syntax/}.
For the purpose of this assignment, we have removed some of the information in the original annotation 
so that the parsing will be based only on information that can be predicted using tools previously explored 
in the course, as shown below.
\begin{center}
\fbox{
\lstinputlisting{code/tree.conll}
}
\end{center}
The format used to represent dependency trees is known as the CoNLL-X format. Part-of-speech tags and lemmas are found in columns 4--5 and 3, respectively. The indices in column 7 encode an unlabeled dependency
tree by pointing to the head of each word, while the labels in column 8 specify the function of each word.%
\footnote{For more information about the CoNLL-X format, see 
{\tt http://ilk.uvt.nl/conll/\#dataformat}. UD actually 
uses an evolved version of CoNLL-X called CoNLL-U, but the differences are irrelevant for this assignment.}

\newsec{Install MaltParser}{malt}%
Go to the MaltParser website at {\tt http://maltparser.org}. Go to the Download page and download the latest version in {\tt maltparser-1.8.1.tar.gz}. Open a terminal and unpack the compressed archive by running:
\begin{small}
\begin{verbatim}
tar xvf maltparser-1.8.1.tar.gz
\end{verbatim}
\end{small}
Then change directory:
\begin{small}
\begin{verbatim}
cd maltparser-1.8.1
\end{verbatim}
\end{small}
Then run MaltParser:
\begin{small}
\begin{verbatim}
java -jar maltparser-1.8.1.jar
\end{verbatim}
\end{small}
This should produce output starting as follows:
%\newpage
\begin{small}
\begin{verbatim}
-----------------------------------------------------------------------------
                          MaltParser 1.8.1                             
-----------------------------------------------------------------------------
         MALT (Models and Algorithms for Language Technology) Group          
             Vaxjo University and Uppsala University                         
                             Sweden                                          
-----------------------------------------------------------------------------
\end{verbatim}
\end{small}
If you don't get this output, notify the the lab instructor.

\newsec{Train a parser}{train}%
To train a parser, run a command like the following:
\begin{small}
\begin{verbatim}
java -jar -Xmx2g maltparser-1.8.1.jar -c myparser -m learn -i ewt-train.conll 
\end{verbatim}
\end{small}
This command can be broken down into two parts, where the first is:
\begin{small}
\begin{verbatim}
java -jar -Xmx2g maltparser-1.8.1.jar
\end{verbatim}
\end{small}
This tells the Java Virtual Machine on your computer to run MaltParser
with a heapspace of 2GB. (The heapspace is set by the flag {\tt
  -Xmx2g}. If you forget this flag, the process will probably run out
of memory.)  The rest of the command consists of specific MaltParser
parameters:

\vspace{0.2cm}
\noindent
\begin{tabular}{lp{15.5cm}}
{\tt -c} &The name of the parsing model you are creating, which can be anything you like. If you name your model {\tt myparser}, it will be saved in a file could {\tt myparser.mco}.\\
{\tt -m} &The processing mode. This should be learn when you are training a model.\\
{\tt -i} &The input file. This should be the name of your training set ({\tt ewt-train.conll} in our example).\\
\end{tabular}

\vspace{0.2cm}
\noindent
Training a model should take somewhere between 10 seconds and a few
minutes, depending on the size of the training set. If it takes longer
(or crashes), notify the lab instructor.\footnote{%
  MaltParser has a large number of options that can be used to tune
  the parser. For more information, see the MaltParser website.}

\newsec{Parse some new data}{parse}%
To parse a new set of sentences using the trained parser, run a
command like the following:
\begin{footnotesize}
\begin{verbatim}
java -jar -Xmx2g maltparser-1.8.1.jar -c myparser -m parse -i ewt-dev.conll -o out.conll
\end{verbatim}
\end{footnotesize} 
The first part of the command is the same as for training, telling the
JVM to run MaltParser, but the MaltParser parameters are slightly
different:

\vspace{0.2cm}
\noindent
\begin{tabular}{lp{15cm}} {\tt -c} &The name of the parsing model,
  which has to be trained beforehand.
  For example, if we specify {\tt myparser}, the model saved as {\tt myparser.mco} will be used. \\
  {\tt -m} &The processing mode. This should be parse when you are parsing new data. \\
  {\tt -i}	 &The input file to be parsed ({\tt ewt-dev.conll} in our example).\\
  {\tt -o} &The output file where parses will be saved
  ({\tt out.conll} in our example).\\
\end{tabular}

\newsec{Evaluate the parser}{eval}%
To evaluate the parse results, you first need an evaluation tool. We
are going to use {\tt MaltEval.jar}, which you can copy from {\tt
  /local/kurs/nlp/syntax/}. You can then evaluate your parser by
running:
\begin{small}
\begin{verbatim}
java -jar -Xmx2g MaltEval.jar -g ewt-dev.conll -s out.conll
\end{verbatim}
\end{small}
The output given is the labeled attachment score (LAS) and the number
of tokens included in the evaluation. There are a number of flags that
can be used to get more output from MaltEval. For example, to get
precision and recall for different dependency relations, add the
flag:\footnote{For more information about the options available, see
  the MaltEval user guide, available on the MaltParser website.}
\begin{small}
\begin{verbatim}
--GroupBy Deprel:all
\end{verbatim}
\end{small}

\newsec{Do an error analysis}{err}%
MaltEval can also be used to visualize dependency trees with color
coding of parse errors. To visualize the parse trees produced earlier,
run:
\begin{small}
\begin{verbatim}
java -jar -Xmx2g MaltEval.jar -g en-ud-dev.conllu -s en-ud-dev-parsed.conllu -v 1
\end{verbatim}
\end{small}
Use the buttons next to the list of the sentences at the bottom to
navigate either to the next or previous sentence or to the next or
previous parse error.
%You can also use the search tool to search for different linguistic phenomena specified by particular part-of-speech tags or dependency labels. For example, to search for sentences containing an instance of the {\tt dobj} relation, select {\tt Gold-standard} from the menu 
%{\tt Search in:}, select {\tt Deprel} from the menu {\tt Search by:}, select {\tt dobj} from the menu 
%{\tt Search for:}, and push the {\tt Search} button.
Go through 5 sentences and discuss the parse errors you see.

\newsec{Submit the assignment}{sub}%
Upload the following to {\it Studentportalen} \textbf{before 20:00h
  December 9th}. In order to pass you must provide answers to all
questions and achieve at least 15/20 points.
\begin{itemize}[noitemsep,topsep=0.2cm]
\item An error analysis based on (at least) 5 sentences (\texttt{.pdf}
  format written in \LaTeX).
\item \textbf{[VG]}: In order to get the grade VG, reach at least 18
  points in the exercises above.
\end{itemize}

\end{document}