
\documentclass[11pt]{article}
%\usepackage[top=20mm,left=20mm,right=20mm,bottom=15mm,a4paper]{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\usepackage[top=20mm,left=20mm,right=20mm,bottom=15mm,headsep=15pt,footskip=15pt,a4paper]{geometry} % see geometry.pdf on how to lay out the page. There's lots.
%\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry
\usepackage[round]{natbib}
\setlength{\bibsep}{0.0pt}
\usepackage{color}
\usepackage{times}
%\usepackage[T1]{fontenc}
%\usepackage{mathptmx}
\usepackage{tikz-dependency}
\usepackage{enumitem}
%\usepackage{times}

\usepackage[procnames]{listings}
\usepackage{color}
 
 

% See the ``Article customise'' template for come common customisations
\newcommand{\refeq}[1]{Equation~\ref{eq:#1}}
\newcommand{\reffig}[1]{Figure~\ref{fig:#1}}
\newcommand{\reftab}[1]{Table~\ref{tab:#1}}
\newcommand{\refsec}[1]{\textsection\ref{sec:#1}}
\newcommand{\newsec}[2]{\section{#1}\label{sec:#2}\noindent}
\newcommand{\newsubsec}[2]{\subsection{#1}\label{sec:#2}\noindent}
\newcommand{\argmax}{\operatornamewithlimits{argmax}} 
\newcommand{\argmin}{\operatornamewithlimits{argmin}} 

\makeatletter         
\def\@maketitle{   % custom maketitle 
\begin{center}%
{\bfseries \@title}%
{\bfseries \@author}%
\end{center}%
\smallskip \hrule \bigskip }

% custom section 
\renewcommand{\section}{\@startsection
{section}%                   % the name
{1}%                         % the level
{0mm}%                       % the indent
{-0.8\baselineskip}%            % the before skip
{0.3\baselineskip}%          % the after skip
{\bfseries\large}}% the style

% custom subsection 
\renewcommand{\subsection}{\@startsection
{subsection}%                   % the name
{2}%                         % the level
{0mm}%                       % the indent
{-0.8\baselineskip}%            % the before skip
{0.3\baselineskip}%          % the after skip
{\bfseries\large}}% the style

\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{1.5ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}\makeatother

%\title{{\LARGE Universal Parser (UP)}\\[-8mm]
%\includegraphics[height=8mm]{RUPA}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\includegraphics[height=8mm]{RUPA}}
\title{{\LARGE Natural Language Processing}\\[1.5mm]{\large Assignment 5: Part-of-Speech Tagging}}
\author{}
\date{} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
 
\lstset{language=Python, 
        basicstyle=\ttfamily\small, 
        keywordstyle=\color{keywords},
        commentstyle=\color{comments},
        stringstyle=\color{red},
        showstringspaces=false,
        identifierstyle=\color{green},
        procnamekeys={def,class}}

\maketitle
%\tableofcontents
%\vspace{3mm}
\vspace{-2mm} \newsec{Introduction}{intro}%
In this assignment, we are going to train and evaluate a statistical
part-of-speech tagger for English. We will make use of HunPoS, an
open-source implementation of the standard HMM tagging model, which is
installed on our Linux system.

\newsec{Data}{data}%
The data sets we are going to use come from the English Web
Treebank.\footnote{The English Web Treebank
  (https://catalog.ldc.upenn.edu/LDC2012T13) contains syntactically
  annotated texts taken from sources such as weblogs, reviews,
  question-answers, newsgroups, and email.  We use the version
  distributed by the Universal Dependencies project
  (http://universaldependencies.github.io/docs/).} The training set
{\tt ewt-train-wt.txt} contains sentences like the following,
separated by blank lines:
\begin{quote}
\begin{tt}
\begin{tabular}{ll}
But          &CONJ\\
in            &ADP\\
my            &PRON\\
view          &NOUN\\
it            &PRON\\
is            &VERB\\
highly        &ADV\\
significant   &ADJ\\
.             &PUNCT\\
\end{tabular}
\end{tt}
\end{quote}
Each line contains a word and its part-of-speech, separated by a
tab. The tag set consists of 17 coarse-grained categories taken from
the Unviersal Dependencies project and listed in the following table.
	\begin{quote}
	\renewcommand{\tabcolsep}{4pt}
	\begin{tabular}{|ll|ll|ll|}
	\hline
	\multicolumn{2}{|l|}{{\bf Open class words}} & \multicolumn{2}{l|}{{\bf Closed class words}} & \multicolumn{2}{l|}{{\bf Other}} \\
	\hline
	ADJ & adjective & ADP & preposition/postposition & PUNCT & punctuation\\
	ADV & adverb & AUX & auxiliary verb & SYM & symbol \\
	INTJ & interjection & CONJ & coordinating conjunction & X & unspecified \\
	NOUN & noun & DET & determiner & & \\
	PROPN & proper noun & NUM & numeral & & \\
	VERB & verb & PART & particle  & & \\
	 & & PRON & pronoun & & \\
	 & & SCONJ & subordinating conjunction & & \\
	\hline
	\end{tabular}
	\end{quote}
        The development set, which will be used to evaluate the tagger, comes in two versions:\\ {\tt ewt-dev-w.txt} contains only words and will be used as input to the tagger;\\
        {\tt ewt-dev-wt.txt} contains both words and tags and will be
        used for comparison when evaluating the output of the tagger.
        All the files needed for the assignment can be found in {\tt
          /local/kurs/nlp/tagging/}.

        \newsec{Train a tagger}{train}%
        To train a tagger using {\tt ewt-train-wt.txt}, you need to
        run a command like:
\begin{verbatim}
hunpos-train mytagger < ewt-train-wt.txt
\end{verbatim}
The trained tagger will be stored in the file {\tt mytagger} (or
whatever name you choose for your tagger).

\newsec{Run a tagger}{run}%
Once you have trained a tagger, you can run it on the development set
as follows:
\begin{verbatim}
hunpos-tag mytagger < ewt-dev-w.txt > ewt-dev-out.txt
\end{verbatim}
The tagged output will be stored in the file {\tt ewt-dev-out.txt}.

\newsec{Evaluate a tagger (3 P)}{eval}%
To evaluate the output of the tagger, you run the evaluation script {\tt score.py}:
\begin{verbatim}
python3 score.py tag ewt-dev-wt.txt ewt-dev-out.txt
\end{verbatim}
In addition to reporting the overall \emph{accuracy} (the percentage
of tokens that were tagged correctly), the script prints a table
showing the precision and recall for different part-of-speech tags. To
give an example, say the precision for the NOUN tag is X\%. This means
that X\% of all words that have been tagged as a noun are in fact
nouns. Now say the recall is Y\%. This means that Y\% of all real
nouns in the text have been correctly tagged as nouns by our tagger.
%the precision for the NOUN tag is the percentage of words tagged as nouns that are really nouns.
%The recall is instead the percentage of real nouns that are also tagged as nouns.

\newsec{Use a shellscript (3 P)}{shell}%
Shellscripts are used to run several command lines after each other in one single script. We introduce their usage here, because you will run the three commands that were introduced in the previous three sections, namely \emph{train}, \emph{run} and \emph{score} a tagger several times in order to find an optimal setting for your tagger in the next exercise. Shellscripts allow you to better keep track of different parameter settings you used and the results these settings gave. In \texttt{/local/kurs/nlp/tagging/} we give you a shellscript named baseline\_script.sh. Have a look at the script and try to understand what it does. Then run it using the following command:\\
\texttt{sh baseline\_script.sh}
%
%\begin{verbatim}
%#!/usr/bin/bash
%
%# name of the current tagger experiment
%EXP=baseline
%
%# train tagger
%hunpos-train $EXP_tagger < ewt-train-wt.txt
%
%# run tagger
%hunpos-tag $EXP_tagger < ewt-dev-w.txt > ewt-dev-$EXP.txt
%
%# score tagger
%pythong3 score.py tag ewt-dev-wt.txt ewt-dev-$EXP.txt > result_$EXP.txt
%
%\end{verbatim}

\newsec{Tune a tagger (7 P)}{tune}%
The HunPoS implementation of HMM tagging has a few parameters that can
be tuned for better tagging. In particular, the {\tt -t} flag can be
used to set the number of tags that are taken as context in the
contextual probabilities. The default is {\tt -t $\!\!$2}, which means
that we train a trigram model. See what happens if you instead use
{\tt -t $\!\!$1} or {\tt -t $\!\!$3} when training the tagger. This
means you copy the shellscript into a new file (e.g. using \texttt{cp
  baseline\_script.sh t1\_script.sh}, rename the experiment (e.g. 'T1'
instead of 'baseline')\footnote{It is most helpful if you give the
  script file and the experiment within the script the same name and
  if that name correlated with the parameter setting.} and add the
parameter \texttt{-t 1} to the training command in the script:
\begin{verbatim}
hunpos-train $EXP_tagger -t 1 < ewt-train-wt.txt
\end{verbatim}
For other parameters, consult the HunPoS user manual.\footnote{https://code.google.com/p/hunpos/wiki/UserManualI}\\
For each new parameter setting you try, copy the shellscript, and
\textbf{(important!)} rename the experiment at the top of the script
(otherwise old results will be overwritten!).

\newsec{Error analysis (7 P)}{error}%
Once you have tuned the tagger, you should perform a manual error
analysis where you go through at least 5 sentences and comment on the
errors made by the tagger.  Are the mistagged words genuinely
ambiguous? Why do you think they were mistagged? Is it possible that
some of the words are mistagged in the gold standard?

\newsec{Submit the assignment}{submit}%
Submit the following to {\tt nlp-course@stp.lingfil.uu.se}
\textbf{before 20:00h November 25th}. In order to pass you must
provide answers to all exercises and achieve at least 15/20 points.
\begin{itemize}[noitemsep,topsep=0.2cm]
\item The output of {\tt score.py} when run on the data as is (exercise 5)
\item The shellscript you used for your final best model (exercise 6)
\item The output of {\tt score.py} for your final best model (exercise 7).
\item  Explain which parameters you used in your final best model (exercise 7)
and give us a short discussion based on your manual error analysis (exercise 8, max 1 page).
\item \textbf{[VG]} In order to get the grade VG, reach at least 18
  points in the exercises above. In addition, dive deeper into shell
  scripting and submit us a more sophisticated shell script which uses
  at least input variables to modify the parameters in the script
  and/or loops to run different parameter settings within one script.
  % \item \textbf{[VG]} In order to get the grade VG, reach at least
  %   18 points in the exercises above, and additionally describe two
  %   more parameter settings you used on your way to find the best
  %   model, give their scores and short explanations as to why you
  %   think they perform worse than the final model (max 1 additional
  %   page).
\end{itemize}

\end{document}