
\documentclass[11pt]{article}
%\usepackage[top=20mm,left=20mm,right=20mm,bottom=15mm,a4paper]{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\usepackage[top=20mm,left=20mm,right=20mm,bottom=15mm,headsep=15pt,footskip=15pt,a4paper]{geometry} % see geometry.pdf on how to lay out the page. There's lots.
%\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry
\usepackage[round]{natbib}
\setlength{\bibsep}{0.0pt}
\usepackage{color}
\usepackage{times}
%\usepackage[T1]{fontenc}
%\usepackage{mathptmx}
\usepackage{tikz-dependency}
\usepackage{enumitem}
%\usepackage{times}

\usepackage[procnames]{listings}
\usepackage{color}
\usepackage{todonotes}

\newenvironment{titlemize}[1]{%
    \paragraph{#1}
    \begin{itemize}
        \setlength\itemsep{0pt}}
    {\end{itemize}}
 
 

% See the ``Article customise'' template for come common customisations
\newcommand{\refeq}[1]{Equation~\ref{eq:#1}}
\newcommand{\reffig}[1]{Figure~\ref{fig:#1}}
\newcommand{\reftab}[1]{Table~\ref{tab:#1}}
\newcommand{\refsec}[1]{\textsection\ref{sec:#1}}
\newcommand{\newsec}[2]{\section{#1}\label{sec:#2}\noindent}
\newcommand{\newsubsec}[2]{\subsection{#1}\label{sec:#2}\noindent}
\newcommand{\argmax}{\operatornamewithlimits{argmax}} 
\newcommand{\argmin}{\operatornamewithlimits{argmin}} 

\makeatletter         
\def\@maketitle{   % custom maketitle 
\begin{center}%
{\bfseries \@title}%
{\bfseries \@author}%
\end{center}%
\smallskip \hrule \bigskip }

% custom section 
\renewcommand{\section}{\@startsection
{section}%                   % the name
{1}%                         % the level
{0mm}%                       % the indent
{-0.8\baselineskip}%            % the before skip
{0.3\baselineskip}%          % the after skip
{\bfseries\large}}% the style

% custom subsection 
\renewcommand{\subsection}{\@startsection
{subsection}%                   % the name
{2}%                         % the level
{0mm}%                       % the indent
{-0.8\baselineskip}%            % the before skip
{0.3\baselineskip}%          % the after skip
{\bfseries\large}}% the style

\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{1.5ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}\makeatother

%\title{{\LARGE Universal Parser (UP)}\\[-8mm]
%\includegraphics[height=8mm]{RUPA}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\includegraphics[height=8mm]{RUPA}}
\title{{\LARGE Natural Language Processing}\\[1.5mm]{\large Assignment 1: Words}}
\author{}
\date{} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}

\lstset{language=Python, 
        basicstyle=\ttfamily\small, 
        keywordstyle=\color{keywords},
        commentstyle=\color{comments},
        stringstyle=\color{red},
        showstringspaces=false,
        identifierstyle=\color{green},
        procnamekeys={def,class}}

\maketitle
%\tableofcontents
%\vspace{3mm}
\thispagestyle{empty}
\section{Introduction}
\indent This assignment involves material from classroom meetings 1 to 4. You
should have watched the relevant videos, read the relevant chapters in the
textbook and made a serious attempt at completing the relevant labs before you
attempt this assignment. If you feel you have done that and still find the
instructions unclear, you are welcome to email the course teachers and/or go to
office hours to ask for help. The assignment is split into 2 sections --- one
about tokenization, one about language modelling and probability. Each section
is worth 50\% of the points. We expect between half a page and a page for each
section. If you are interested in receiving a VG grade, you may complete the
\textit{optional} exercise available to you, which is related to language
modelling. Note that the exercise must be completed in full in order to receive VG
credit. Please do not submit more than 5 pages overall (excluding figures etc.) You
answers %
for each section should be self-contained.

\section{Tokenization}
In the videos and the book chapters, you learned about regular
expressions (if you did not already know all about them before that
;)). In the labs, you got to play with regular expressions in the
context of tokenization.  You started with a very simple tokenizer and
gradually refined it by looking at the errors it was making.  If you
have not done so already, try to improve it so that it reaches at
least 95\% precision and recall. Describe your improvements. 
What types of errors did you deal with and how? What types of errors 
remain? You may illustrate the problems you dealt with by adding snippets
of code.

You will use a new test set {\tt dev2-raw.txt} (available from {\tt
  /local/kurs/nlp/basic2}) to run your tokenizer on. Write down for
yourself your expectation of how good you think your tokenizer is
going to be before running it.  Compare the output of your system
against {\tt dev2-gold-sent.txt} and discuss the result. Are there
still cases of under/oversplitting that you did not anticipate based 
on your results on dev1? If so, can you identify the
reason? Can you identify difficult challenges?

\section{Language Modelling}
\indent In Lab 3, you learned about how to use MLE for language modelling.
Hopefully, in the lab, you saw its limitations. 
In Lab 4, you experimented with smoothing methods.\\
\indent Define MLE and discuss its limitations. Explain the principles behind
smoothing and how they help circumvent the limitations of MLE. Illustrate this
with examples. (You may take them from the Sherlock Holmes text but use
different examples than those we asked you to look at in the labs.)\\
\indent Describe and compare different smoothing methods.  Illustrate your
answer with results from experiments you made in the lab. You may use the table
you produced during the lab.

\section{VG: Calculating Perplexity of a Language Model}
Consider the following \texttt{TRAIN} and \texttt{TEST} ``corpus'':

\begin{verbatim}
TRAIN: <s> I would much rather eat pizza than ice cream . </s>
TEST: <s> I love anchovies on my pizza . </s>
\end{verbatim}

``Train'' a \textbf{bigram} language model on the \texttt{TRAIN} set and report
its perplexity on the \texttt{TEST} set. Use \texttt{Laplace smoothing} to
account for unseen words. Show your work and report the conditional
probabilities of each bigram, e.g. $P(\mathbf{pizza}|\mathbf{eat}) = ?$.

\section{Grading Criteria}
\begin{titlemize}{Basic Criteria}
\item Answers are given in understandable English.
\item Answers are stated clearly and coherently.
\item Answers are essentially correct.
\end{titlemize}
\begin{titlemize}{Additional Criteria}
\item Answers are well motivated.
\item Answers are well illustrated.
\item Answers reveal extensive knowledge of the textbook chapter(s).
\end{titlemize}
To pass the assignment, you must meet all the basic criteria on all
subparts of the assignment.  To get VG, you must in addition meet some
of the additional criteria for most of subparts.


\section{Submit the assignment}
\noindent
Please submit your assignment though studium as a PDF file without identifying
information (i.e., do not include your name in the report or the file name). It
should follow the style and margins given in the example submission, even if
not created with LaTeX. The deadlines can be found in studium.

%Submit your assignment as a pdf file named
%firstname\_lastname\_assignment\_1.pdf. It should follow the style and margins
%given in the example submission even if not created with LaTeX. See deadline
%on studentportalen.

\end{document}
